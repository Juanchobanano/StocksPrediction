{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarlo Learning Introduction.\n",
    "\n",
    "---\n",
    "\n",
    "I highly recommend to visit David Silver Lectures on Monte-Carlo Reinforcement Learning: https://www.youtube.com/watch?v=PnHCvfgC_ZA&t\n",
    "\n",
    "In contrast of **Dynamic Programming**, **MonteCarlo Learning** methods learn directly from episodes of experience. These methods are model-free, which means they don't need knowledge of MDP transitions / reward functions. MC learns from complete episodes (no bootstrapping). It uses the simplest possible idea: the value of a state is equal to its mean return. \n",
    "\n",
    "In this notebook, we first consider the problem of the policy evaluation using montecarlo learning, then, we continue with policy control.\n",
    "\n",
    "In this section, we will solve the **Black Jack Problem**, hope you enjoy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Evaluation (Prediction or Planning) for Montecarlo Learning.\n",
    "\n",
    "---\n",
    "1. Goal: Learn $V_{\\pi}$ from episodes of experience under the policy $\\pi$, i.e, $S_{1}, A_{1}, R_{2}, ..., S_{k}$ following policy $\\pi$.\n",
    "\n",
    "2. Recall that the return is the total discounted reward:\n",
    "$Gt = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-1} R_{T}$\n",
    "\n",
    "3. Recall that the value function is the expected return: \n",
    "$V_{\\pi}(s) = E_{\\pi}[G_{t} | S_{t} = s]$\n",
    "\n",
    "4. Monte-Carlo policy evalation uses empirical mean return instead of expected return.\n",
    "\n",
    "In this sub-section, we will use the same example used in Dynamic Programming Lecture to introduce the policy evaluation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Graphics module ready!\")\n",
    "        \n",
    "    def render(self, x, y, policy_evaluator, plot_values = True):\n",
    "            \n",
    "        fig1 = plt.figure(figsize=(4, 4))\n",
    "        ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "\n",
    "        # Horizontal lines.\n",
    "        for i in range(0, 6):\n",
    "            ax1.axhline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "            ax1.axvline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "\n",
    "         # Salida, Meta & GameOver.\n",
    "        ax1.add_patch(patches.Rectangle((0.0, 0.0), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.8), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.2), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.6), 0.2, 0.2, facecolor = \"#68FF33\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.8, 0.8), 0.2, 0.2, facecolor = \"#FF5533\"))\n",
    "        \n",
    "        # Muros del juego.\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.4), 0.2, 0.4, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.6, 0.2), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.0), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.4), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        # Limit grid view.\n",
    "        plt.ylim((0, 1))\n",
    "        plt.xlim((0, 1))\n",
    "\n",
    "        # Plot player.\n",
    "        plt.scatter(x, y, s = 100, color = \"black\", marker = \"o\", facecolor = \"blue\", edgecolors = \"blue\", zorder = 10)\n",
    "\n",
    "        # Plot state values.\n",
    "        if plot_values:\n",
    "            for i in range(0, len(policy_evaluator.value_state_table)):\n",
    "                for j in range(0, len(policy_evaluator.value_state_table[0])):\n",
    "                    plt.text(self.environment.grid_pos[i] - 0.08, self.environment.grid_pos[j] - 0.03, \n",
    "                             round(policy_evaluator.value_state_table[i][j], 1), fontsize=10)\n",
    "                \n",
    "        # Plot grid.\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.rw = -1 # Living (Movement) Penalty\n",
    "        self.walls_and_paths = [[1, 1, 1, 1, 1], [0, 1, 0, 0, 1], [1, 1, 1, 1, 0], [1, 0, 1, 1, 1], [1, 1, 0, 1, 1]]\n",
    "        self.rewards = [[self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw]]\n",
    "        self.grid_pos = [0.1, 0.3, 0.5, 0.7, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelFreeAgent():\n",
    "    \n",
    "    def __init__(self, policy, discount_factor):\n",
    "        self.pos = [0,0]\n",
    "        self.total_reward = 0\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Start with a random policy. 0.25 chance of moving to any direction.\n",
    "        self.policy = policy   \n",
    "        \n",
    "    # Move!\n",
    "    def move(self, environment):\n",
    "    \n",
    "        # Select action according to policy.\n",
    "        action = self.policy[self.pos[0]][self.pos[1]]\n",
    "        if action == \"r\":\n",
    "            action = self.actions[random.randint(0, 3)]\n",
    "\n",
    "        # Move to new position according to action taken.\n",
    "        self.pos = self.forwardState(self.pos, action, environment)\n",
    "        \n",
    "    # Select action according to policy.\n",
    "    def selectAction(self, state):\n",
    "        \n",
    "        if(self.policy[state[0]][state[1]] == \"r\"):\n",
    "            action = self.actions[random.randint(0, len(self.available_actions) - 1)] # Agent initial policy.\n",
    "        else: \n",
    "            action = self.policy[state[0]][state[1]]\n",
    "        return action\n",
    "            \n",
    "    def forwardState(self, pos, action, environment):\n",
    "        \n",
    "        # New position array.\n",
    "        new_position = pos\n",
    "        \n",
    "        # Compute new position based on action taken.\n",
    "        if(action == \"up\" and pos[1] < 4):\n",
    "            if(environment.walls_and_paths[pos[0]][pos[1] + 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] + 1]\n",
    "        elif(action == \"down\" and pos[1] > 0):\n",
    "            if(environment.walls_and_paths[pos[0]][pos[1] - 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] - 1]\n",
    "        elif(action == \"left\" and pos[0] > 0):\n",
    "            if(environment.walls_and_paths[pos[0] - 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] - 1, pos[1]]\n",
    "        elif(action == \"right\" and pos[0] < 4):\n",
    "            if(environment.walls_and_paths[pos[0] + 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] + 1, pos[1]]\n",
    "        return new_position\n",
    "\n",
    "    def getPosition(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def getReward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def setPosition(self, x, y):\n",
    "        self.pos = [x, y]\n",
    "        \n",
    "    def selectBestAction(self, value_state_table, environment):\n",
    "        \n",
    "        go_up = self.forwardState(self.pos, \"up\", environment)\n",
    "        go_down = self.forwardState(self.pos, \"down\", environment)\n",
    "        go_left = self.forwardState(self.pos, \"left\", environment)\n",
    "        go_right = self.forwardState(self.pos, \"right\", environment)\n",
    "        \n",
    "        up_value = value_state_table[go_up[0]][go_up[1]]\n",
    "        down_value = value_state_table[go_down[0]][go_down[1]]\n",
    "        left_value = value_state_table[go_left[0]][go_left[1]]\n",
    "        right_value = value_state_table[go_right[0]][go_right[1]]\n",
    "        values = [up_value, down_value, left_value, right_value]\n",
    "        \n",
    "        best_action = self.actions[values.index(max(values))] \n",
    "        return best_action       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluation(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent, ephocs = 10, visit = \"first\"):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent                     \n",
    "        self.ephocs = ephocs\n",
    "        self.value_state_table = [[0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 100, 0]]\n",
    "        self.visit = visit\n",
    "        \n",
    "        # Generate a list for each state.\n",
    "        self.states_list = list()\n",
    "        for i in range(0, 5):\n",
    "            column = list()\n",
    "            for j in range(0, 5):\n",
    "                column.append(list())\n",
    "            self.states_list.append(column)\n",
    "    \n",
    "    def evaluate(self, plot_grid = True):\n",
    "        self.montecarlo_policy_evaluation(self.ephocs, plot_grid)\n",
    "        \n",
    "    def montecarlo_policy_evaluation(self, ephocs, plot_grid):\n",
    "        \n",
    "        # Set agent position.\n",
    "        self.agent.setPosition(0, 0)\n",
    "        \n",
    "        # Initialize list of states & rewards.\n",
    "        list_of_states = list()\n",
    "        list_of_rewards = list()\n",
    "        \n",
    "        # Append current position.\n",
    "        list_of_states.append([0, 0])\n",
    "        \n",
    "        for k in range(0, ephocs):\n",
    "            while not (self.agent.pos[0] == 4 and self.agent.pos[1] == 3):\n",
    "                \n",
    "                # Move the player.\n",
    "                self.agent.move(self.environment)\n",
    "                \n",
    "                # Append new position\n",
    "                list_of_states.append(self.agent.pos)\n",
    "                list_of_rewards.append(self.environment.rewards[self.agent.pos[0]][self.agent.pos[1]])\n",
    "\n",
    "                # Method of the super class.\n",
    "                if(plot_grid):\n",
    "\n",
    "                    # Render game.\n",
    "                    pos = self.agent.getPosition()\n",
    "                    grid_coords = self.environment.grid_pos\n",
    "\n",
    "                    self.render(self.environment.grid_pos[pos[0]], self.environment.grid_pos[pos[1]], self, True)\n",
    "                    time.sleep(0.1)\n",
    "                    clear_output(wait = True)\n",
    "                    \n",
    "            # Update Value State Table with MonteCarlo algorithm.\n",
    "            if self.visit == \"first\":\n",
    "                self.updateValueStateTable_MC_FirstVisit(list_of_states, list_of_rewards)\n",
    "            elif self.visit == \"multiple\":\n",
    "                self.updateValueStateTable_MC_MultipleVisits(list_of_states, list_of_rewards)\n",
    "            \n",
    "            # Set players position at the beginning.\n",
    "            self.agent.setPosition(0, 0)\n",
    "            \n",
    "            # Reset lists.\n",
    "            list_of_states = list()\n",
    "            list_of_rewards = list()\n",
    "            list_of_states.append([0, 0])\n",
    "                    \n",
    "    def updateValueStateTable_MC_FirstVisit(self, list_of_states, list_of_rewards):\n",
    "            \n",
    "        # Initialize discount reward factor. \n",
    "        G = 0\n",
    "        \n",
    "        # Execute montecarlo learning algorithm.\n",
    "        for s in range(0, len(list_of_states) - 1):\n",
    "\n",
    "            # Update cumulative discount return.\n",
    "            G += self.agent.discount_factor * list_of_rewards[len(list_of_states) - 2 - s]\n",
    "\n",
    "            # Check if current state is not in the list.\n",
    "            not_in_list = True\n",
    "            current_state = list_of_states[len(list_of_states) - 2 - s]\n",
    "            states_copy = list_of_states[0: len(list_of_states) - 3 - s].copy()\n",
    "\n",
    "            #print(states_copy)\n",
    "            \n",
    "            for st in states_copy:\n",
    "                if current_state == st:\n",
    "                    not_in_list = False\n",
    "                \n",
    "            # Update state value table.\n",
    "            if (not_in_list) and current_state != [4, 3]:\n",
    "                \n",
    "                # Compute new state value.\n",
    "                array = self.states_list[current_state[0]][current_state[1]]\n",
    "                array.append(G)\n",
    "                average = self.getAverage(array)\n",
    "                self.value_state_table[current_state[0]][current_state[1]] = average\n",
    "                \n",
    "                # Render value state update.\n",
    "                if False:\n",
    "                    self.agent.setPosition(current_state[0], current_state[1])\n",
    "                    pos = self.agent.getPosition()\n",
    "                    self.render(self.environment.grid_pos[pos[0]], self.environment.grid_pos[pos[1]], self, True)\n",
    "                    time.sleep(0.1)\n",
    "                    clear_output(wait = True)\n",
    "\n",
    "    def updateValueStateTable_MC_MultipleVisits(self, list_of_states, list_of_rewards):\n",
    "            \n",
    "        # Initialize discount reward factor. \n",
    "        G = 0\n",
    "        \n",
    "        # Execute montecarlo learning algorithm.\n",
    "        for s in range(0, len(list_of_states) - 1):\n",
    "\n",
    "            # Update cumulative discount return.\n",
    "            G += self.agent.discount_factor * list_of_rewards[len(list_of_states) - 2 - s]\n",
    "            current_state = list_of_states[len(list_of_states) - 2 - s]\n",
    "\n",
    "            # Update state value table.\n",
    "            if current_state != [4, 3]:\n",
    "                \n",
    "                # Compute new state value.\n",
    "                array = self.states_list[current_state[0]][current_state[1]]\n",
    "                array.append(G)\n",
    "                average = self.getAverage(array)\n",
    "                self.value_state_table[current_state[0]][current_state[1]] = average\n",
    "                \n",
    "                # Render value state update.\n",
    "                if False: \n",
    "                    self.agent.setPosition(current_state[0], current_state[1])\n",
    "                    pos = self.agent.getPosition()\n",
    "                    self.render(self.environment.grid_pos[pos[0]], self.environment.grid_pos[pos[1]], self, True)\n",
    "                    time.sleep(0.05)\n",
    "                    clear_output(wait = True)\n",
    "                \n",
    "                \n",
    "    def getAverage(self, array):\n",
    "        average = 0\n",
    "        for num in array: \n",
    "            average += num\n",
    "        return average / len(array)\n",
    "    \n",
    "    def updatePolicy(self):\n",
    "         for i in range(0, len(self.value_state_table)):\n",
    "                for j in range(0, len(self.value_state_table[0])):\n",
    "                    if self.environment.walls_and_paths[i][j] == 1:\n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        best_action = self.agent.selectBestAction(self.value_state_table, self.environment)\n",
    "                        self.agent.policy[i][j] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent             \n",
    "        \n",
    "    def update(self, secs):\n",
    "        \n",
    "        pos = self.agent.getPosition()\n",
    "        grid_coords = self.environment.grid_pos\n",
    "        self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        while not (self.agent.pos[0] == 4 and self.agent.pos[1] == 3):\n",
    "            \n",
    "            self.agent.move(self.environment)\n",
    "            pos = self.agent.getPosition()\n",
    "            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "            \n",
    "            time.sleep(secs)\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "        #self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        #time.sleep(secs)\n",
    "        #print(\"Yuhuu, we won the game!\")\n",
    "        #clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Policy Evaluation - Montecarlo Learning First Visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "\n",
    "# Initialize environment and agent.\n",
    "discount_factor = 0.9\n",
    "environment = GridEnvironment()\n",
    "agent = modelFreeAgent(policy, discount_factor)\n",
    "\n",
    "# Initialize policy evaluation class.\n",
    "ephocs = 2000\n",
    "policy_evaluation = PolicyEvaluation(environment, agent, ephocs, visit = \"first\")\n",
    "policy_evaluation.evaluate(plot_grid = False)\n",
    "policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win the Game with the previous policy evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'right'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [1,4]\n",
    "game = Game(environment, agent)\n",
    "game.update(secs = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [4, 1]\n",
    "game = Game(environment, agent)\n",
    "game.update(secs = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [0, 0]\n",
    "game = Game(environment, agent)\n",
    "game.update(secs = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2  Policy Evaluation - Montecarlo Learning Multiple Visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "\n",
    "# Initialize environment and agent.\n",
    "discount_factor = 0.9\n",
    "environment2 = GridEnvironment()\n",
    "agent2 = modelFreeAgent(policy, discount_factor)\n",
    "\n",
    "# Initialize policy evaluation class.\n",
    "ephocs = 1000\n",
    "policy_evaluation = PolicyEvaluation(environment2, agent2, ephocs, visit = \"multiple\")\n",
    "policy_evaluation.evaluate(plot_grid = False)\n",
    "policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win the Game with the previous policy evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'right'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [4,1]\n",
    "game = Game(environment, agent)\n",
    "game.update(secs = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [1, 4]\n",
    "game = Game(environment, agent)\n",
    "game.update(secs = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAFQ0lEQVR4nO3bsWuTeRzH8W+0KViK/QMEF/EGh7gEBA9XQTehgqC3uh8O/RsqyC2ObqcgWHBTcD9ByGIW4cRF8A8oFoVG77mpyJ3Pk4jXmE+412t8fk/Jl5B3vmlpek3TFJDnyKIHANqJE0KJE0KJE0KJE0KtTDscDIb+lAtzNh6Pem3Xp8Y57QfTHLyRvNjZXfQoM53b3KiqqtWt0YIn+Tb728OqWo7XwsHrYBlmrZq+AH2shVDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFC9pmk6DweDYfchcCjG41Gv7brNCaFWZt3QVXWagy2/ujVa9Cgz7W8Pq2r5nttlmPdg1p/Hi57k2/wx6D6zOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCHUzO9zwv/Z7psT9fLO9Xp9/3JN9taqv/6hTt94UmdvPaiNU+/m+tg2J3R4+/R8PRo8rFf3rtTk/XpVc6Qm79fr1b0r9WjwsN4+PT/XxxcntNh9c6KebW7Xpw/Hqpn0/3HWTPr16cOxera5XbtvTsxtBnFCi5d3rtfnyfTf+j5PVmr82/W5zSBOaPH6/uWvNua/NZN+/fn75bnNIE5oMdlbO9T7voc4oUV//cOh3vc9xAktTt94Ur3+ZOo9vf6kfvrlydxmECe0OHvrQR3tf5p6z9H+pxr8+mBuM4gTWmycelcXd7ZqZe3jVxu015/UytrHurizNdd/RBAndDh56XldHV+rMzcfV//4XtWRv6p/fK/O3HxcV8fX6uSl53N9fP++B1NsnHpXF+7ergt3b//wx7Y5IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IVSvaZrOw8Fg2H0IHIrxeNRru25zQqiZ3+fsqjrNwZZf3RotepSZ9reHVbV8z+0yzLtMr4OqL6+FNjYnhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhOo1TdN5OBgMuw+BQzEej3pt121OCLUy64auqtMcbPllmHeZZq36Mu/q1mjRo8y0vz2sqqoXO7sLnuTbnNvc6DyzOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCFUr2mazsPBYNh9CByK8XjUa7tuc0KolVk3vNjZ/RFz/GfnNjeqqmp1a7TgSWbb3x5WVfc7ZpqDT1DLMO8yzVo1/dOpzQmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhek3TdB4OBsPuQ+BQjMejXtv1qXECi+NjLYQSJ4QSJ4QSJ4QSJ4QSJ4T6G5ym55mNyI60AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [0, 0]\n",
    "game = Game(environment, agent)\n",
    "game.update(secs = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
