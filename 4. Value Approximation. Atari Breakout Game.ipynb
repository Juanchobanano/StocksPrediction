{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Breakout with Deep Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import os \n",
    "import pickle \n",
    "import random\n",
    "import zlib\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Juan Esteban Cepeda\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize constants.\n",
    "resume = True  # resume training from checkpoint (if exists) \n",
    "CHECKPOINT_PATH = 'deep_q_breakout_path_7' \n",
    "MB_SIZE = 32  # mini batch size \n",
    "ER_BUFFER_SIZE = 1000000  # experience relay (ER) buffer size \n",
    "COMPRESS_ER = True  # compress episodes in the EP buffer \n",
    "EXPLORE_STEPS = 1000000  # frames over which to anneal epsilon \n",
    "EPSILON_START = 1.0  # starting chance of an action being random \n",
    "EPSILON_END = 0.1  # final chance of an action being random \n",
    "STATE_FRAMES = 4  # number of frames to store in the state \n",
    "SAVE_EVERY_X_STEPS = 10000  # how often to save the model on the disk \n",
    "UPDATE_Q_NET_FREQ = 1  # how often to update the q network \n",
    "UPDATE_TARGET_NET_EVERY_X_STEPS = 10000  # copy the q-net weights to the target net \n",
    "DISCOUNT_FACTOR = 0.99  # discount factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():    \n",
    "    \n",
    "    \"\"\"Initialize the session, the networks, and the environment\"\"\"    \n",
    "    # Create environment    \n",
    "    env = gym.envs.make(\"BreakoutDeterministic-v4\")\n",
    "    tf.reset_default_graph()\n",
    "    session = tf.Session()\n",
    "    \n",
    "    # Tracks the total number of training steps    \n",
    "    tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    # Create q- and target- networks    \n",
    "    q_network = build_network(\"q_network\")    \n",
    "    t_network = build_network(\"target_network\")\n",
    "    \n",
    "    # create the operations to copy the q-net weights to the t-net    \n",
    "    q_net_weights = [t for t in tf.trainable_variables() if t.name.startswith(q_network.scope)]    \n",
    "    q_net_weights = sorted(q_net_weights, key=lambda v: v.name)    \n",
    "    t_net_weights = [t for t in tf.trainable_variables() if t.name.startswith(t_network.scope)]    \n",
    "    t_net_weights = sorted(t_net_weights, key=lambda v: v.name)\n",
    "    t_net_updates = [n2_v.assign(n1_v) for n1_v, n2_v in zip(q_net_weights, t_net_weights)]\n",
    "    \n",
    "    # pre-processor of game frames    \n",
    "    frame_proc = frame_preprocessor()\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(0.00025)    \n",
    "    # optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "    \n",
    "    # training op    \n",
    "    train_op = optimizer.minimize(q_network.loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # restore checkpoint    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if not os.path.exists(CHECKPOINT_PATH):     \n",
    "        os.mkdir(CHECKPOINT_PATH)\n",
    "        \n",
    "    checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\n",
    "    \n",
    "    if resume and checkpoint:        \n",
    "        session.run(tf.global_variables_initializer())        \n",
    "        session.run(tf.local_variables_initializer())\n",
    "        print(\"\\nRestoring checkpoint...\")        \n",
    "        saver.restore(session, checkpoint.model_checkpoint_path)    \n",
    "    else:        \n",
    "        session.run(tf.global_variables_initializer())        \n",
    "        session.run(tf.local_variables_initializer())\n",
    "    \n",
    "    return session, q_network, t_network, t_net_updates, frame_proc, saver, train_op, env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(scope: str, input_size=84, num_actions=4): \n",
    "    \n",
    "    \"\"\"Builds the network graph.\"\"\"\n",
    "    with tf.variable_scope(scope): \n",
    "        \n",
    "        # Our input are STATE_FRAMES grayscale frames of shape 84, 84 each        \n",
    "        input_placeholder = tf.placeholder(dtype=np.float32, shape = [None, input_size, input_size, STATE_FRAMES])                                      \n",
    "        normalized_input = tf.to_float(input_placeholder) / 255.0\n",
    "        \n",
    "        # action prediction        \n",
    "        action_placeholder = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "        \n",
    "        # target action        \n",
    "        target_placeholder = tf.placeholder(dtype=np.float32, shape=[None])\n",
    "        \n",
    "        # Convolutional layers       \n",
    "        conv_1 = tf.layers.conv2d(normalized_input, 32, 8, 4, activation=tf.nn.relu)        \n",
    "        conv_2 = tf.layers.conv2d(conv_1, 64, 4, 2, activation=tf.nn.relu)        \n",
    "        conv_3 = tf.layers.conv2d(conv_2, 64, 3, 1, activation=tf.nn.relu)\n",
    "        \n",
    "        # Fully connected layers        \n",
    "        flattened = tf.layers.flatten(conv_3)        \n",
    "        fc_1 = tf.layers.dense(flattened, 512, activation = tf.nn.relu)\n",
    "        \n",
    "        q_estimation = tf.layers.dense(fc_1, num_actions)\n",
    "        \n",
    "        # Get the predictions for the chosen actions only        \n",
    "        batch_size = tf.shape(normalized_input)[0]        \n",
    "        gather_indices = tf.range(batch_size) * tf.shape(q_estimation)[1] + action_placeholder        \n",
    "        action_predictions = tf.gather(tf.reshape(q_estimation, [-1]), gather_indices)\n",
    "        \n",
    "        # Calculate the loss        \n",
    "        loss = tf.losses.huber_loss(labels=target_placeholder, predictions=action_predictions, reduction=tf.losses.Reduction.MEAN)\n",
    "    \n",
    "    Network = namedtuple('Network', (\n",
    "                         'scope', \n",
    "                         'input_placeholder', \n",
    "                         'action_placeholder', \n",
    "                         'target_placeholder',                         \n",
    "                         'q_estimation',\n",
    "                         'action_predictions',\n",
    "                         'loss'))\n",
    "    \n",
    "    return Network(scope=scope, \n",
    "                   input_placeholder=input_placeholder, \n",
    "                   action_placeholder=action_placeholder,                   \n",
    "                   target_placeholder=target_placeholder,                   \n",
    "                   q_estimation=q_estimation,                  \n",
    "                   action_predictions=action_predictions,                   \n",
    "                   loss=loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_preprocessor():   \n",
    "    \"\"\"Pre-processing the input data\"\"\"\n",
    "    with tf.variable_scope(\"frame_processor\"):        \n",
    "        input_placeholder = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)       \n",
    "        processed_frame = tf.image.rgb_to_grayscale(input_placeholder)        \n",
    "        processed_frame = tf.image.crop_to_bounding_box(processed_frame, 34, 0, 160, 160)        \n",
    "        processed_frame = tf.image.resize_images(processed_frame, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        processed_frame = tf.squeeze(processed_frame)\n",
    "        \n",
    "    FramePreprocessor = namedtuple('FramePreprocessor', 'input_placeholder processed_frame')\n",
    "    return FramePreprocessor(        \n",
    "            input_placeholder=input_placeholder,        \n",
    "            processed_frame=processed_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_action(state, net, epsilon):    \n",
    "    \"\"\"Epsilon-greedy policy\"\"\"\n",
    "    \n",
    "    # choose an action given our last state    \n",
    "    tmp = np.ones(env.action_space.n, dtype=float) * epsilon / env.action_space.n    \n",
    "    q_estimations = session.run(net.q_estimation, \n",
    "                feed_dict={net.input_placeholder: np.reshape(state, (1,) + state.shape)})[0]\n",
    "    tmp[np.argmax(q_estimations)] += (1.0 - epsilon)\n",
    "    new_action = np.random.choice(np.arange(len(tmp)), p=tmp)\n",
    "    \n",
    "    return new_action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_experience_replay_buffer(buffer: deque, initial_buffer_size: int): \n",
    "    \"\"\"Initial population of the experience replay buffer\"\"\"\n",
    "     \n",
    "    # Initialize epsilon based on the current step \n",
    "    epsilon_step = (EPSILON_START - EPSILON_END) / EXPLORE_STEPS \n",
    "    epsilon = max(EPSILON_END, EPSILON_START - session.run(tf.train.get_global_step()) * epsilon_step)\n",
    " \n",
    "    # Populate the replay memory with initial experience \n",
    "    state = env.reset() \n",
    "    state = session.run(frame_proc.processed_frame, \n",
    "                        feed_dict={frame_proc.input_placeholder: state})\n",
    "    \n",
    "    state = np.stack([state] * STATE_FRAMES, axis=2)\n",
    "    for i in range(initial_buffer_size):\n",
    "        \n",
    "        # Sample next state with the q_network \n",
    "        action = choose_next_action(state, q_network, epsilon)\n",
    "\n",
    "        # Perform one action step \n",
    "        next_state, reward, terminal, info = env.step(action) \n",
    "        next_state = session.run(frame_proc.processed_frame, feed_dict={frame_proc.input_placeholder: next_state})\n",
    "     \n",
    "        # Stack the game frames in a single array \n",
    "        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        \n",
    "        # Store the experience in ER \n",
    "        if COMPRESS_ER: \n",
    "            buffer.append(\n",
    "                zlib.compress(\n",
    "                    pickle.dumps((state, action, reward, next_state, terminal), 2), 2)) \n",
    "        else: \n",
    "            buffer.append((state, action, reward, next_state, terminal))\n",
    "            \n",
    "        # Set next state as current\n",
    "        if terminal:\n",
    "            state = env.reset() \n",
    "            state = session.run(frame_proc.processed_frame, \n",
    "                                feed_dict={frame_proc.input_placeholder: state})\n",
    "            state = np.stack([state] * STATE_FRAMES, axis=2)\n",
    "        else: \n",
    "            state = next_state\n",
    "            \n",
    "        print(\"\\rExperience replay buffer: {} / {} initial ({} total)\".format( len(buffer), initial_buffer_size, buffer.maxlen), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning():    \n",
    "    \n",
    "    \"\"\"The Q-learning training process\"\"\"\n",
    "    # build experience replay    \n",
    "    observations = deque(maxlen=ER_BUFFER_SIZE)\n",
    "    print(\"Populating replay memory...\")    \n",
    "    populate_experience_replay_buffer(observations, 100000)\n",
    "    \n",
    "    # initialize statistics    \n",
    "    stats = namedtuple('Stats', 'rewards lengths')(rewards=list(), lengths=list())    \n",
    "    global_time = session.run(tf.train.get_global_step())    \n",
    "    time = 0\n",
    "    episode = 1\n",
    "    episode_reward = 0    \n",
    "    global_reward = 0\n",
    "    \n",
    "    # Start the training with an initial state    \n",
    "    state = env.reset()    \n",
    "    state = session.run(frame_proc.processed_frame,\n",
    "                        feed_dict={frame_proc.input_placeholder: state})    \n",
    "    state = np.stack([state] * STATE_FRAMES, axis=2)\n",
    "    \n",
    "    while True:     \n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        # Initialize epsilon based on the current step        \n",
    "        epsilon_step = (EPSILON_START - EPSILON_END) / EXPLORE_STEPS        \n",
    "        epsilon = max(EPSILON_END, EPSILON_START - (global_time - 1) * epsilon_step)\n",
    "        \n",
    "        # Copy q-net weights to the target-net        \n",
    "        if global_time % UPDATE_TARGET_NET_EVERY_X_STEPS == 0:            \n",
    "            session.run(t_net_updates)            \n",
    "            print(\"\\nCopied model parameters to target network.\")\n",
    "            \n",
    "        # Sample next action        \n",
    "        action = choose_next_action(state, q_network, epsilon)\n",
    "        \n",
    "        # Perform one step with the selected action        \n",
    "        next_state, reward, terminal, info = env.step(action)\n",
    "        \n",
    "        # This is how we pre-process        \n",
    "        next_state = session.run(frame_proc.processed_frame, \n",
    "                                 feed_dict={frame_proc.input_placeholder: next_state})\n",
    "        # Stack the game frames in a single array        \n",
    "        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        \n",
    "        # Store the experience in ER        \n",
    "        if COMPRESS_ER:            \n",
    "            observations.append(                \n",
    "                zlib.compress(pickle.dumps((state, action, reward, next_state, terminal), 2), 2))        \n",
    "        else:            \n",
    "            observations.append((state, action, reward, next_state, terminal))\n",
    "            \n",
    "        # Sample a mini-batch from the experience replay memory        \n",
    "        mini_batch = random.sample(observations, MB_SIZE)       \n",
    "        if COMPRESS_ER:            \n",
    "            mini_batch = [pickle.loads(zlib.decompress(comp_item)) for comp_item in mini_batch]\n",
    "            \n",
    "        states_batch, action_batch, reward_batch, next_states_batch, terminal_batch = map(np.array, zip(*mini_batch))\n",
    "        \n",
    "        if global_time % UPDATE_Q_NET_FREQ == 0:            \n",
    "            # Compute next q values using the target network            \n",
    "            q_values_next = session.run(t_network.q_estimation,                \n",
    "                                        feed_dict={t_network.input_placeholder: next_states_batch})\n",
    "            \n",
    "        # Calculate q values and targets \n",
    "        targets_batch = (reward_batch + \n",
    "                         np.invert(terminal_batch).astype(np.float32) * \n",
    "                        DISCOUNT_FACTOR * \n",
    "                        np.amax(q_values_next, axis=1))\n",
    "\n",
    "        # Perform gradient descent update \n",
    "        states_batch = np.array(states_batch)\n",
    "        _, loss = session.run([train_op, q_network.loss],                                  \n",
    "                              feed_dict={                                      \n",
    "                                  q_network.input_placeholder: states_batch,\n",
    "                                  q_network.action_placeholder: action_batch,\n",
    "                                  q_network.target_placeholder: targets_batch})\n",
    "\n",
    "        episode_reward += reward        \n",
    "        global_reward += reward        \n",
    "        time += 1        \n",
    "        global_time += 1\n",
    "\n",
    "        print(\"\\rEpisode {}: \"\n",
    "              \"time {:5}; \"\n",
    "              \"reward {}; \"\n",
    "              \"epsilon: {:.4f}; \" \n",
    "              \"loss: {:.6f}; \"\n",
    "              \"@ global step {} \"\n",
    "              \"with total reward {}\".format(            \n",
    "                  episode,            \n",
    "                  time,            \n",
    "                  episode_reward,            \n",
    "                  epsilon,            \n",
    "                  loss,           \n",
    "                  global_time,            \n",
    "                  global_reward), end=\"\")\n",
    "\n",
    "        if terminal:         \n",
    "\n",
    "            # Episode end\n",
    "            print()\n",
    "\n",
    "            stats.rewards.append(int(episode_reward))            \n",
    "            stats.lengths.append(time)\n",
    "\n",
    "            time = 0            \n",
    "            episode_reward = 0           \n",
    "            episode += 1\n",
    "\n",
    "            state = env.reset()            \n",
    "            state = session.run(frame_proc.processed_frame, \n",
    "                                feed_dict={frame_proc.input_placeholder: state})            \n",
    "            state = np.stack([state] * STATE_FRAMES, axis=2)        \n",
    "\n",
    "        else:            \n",
    "\n",
    "            # Set next state as current            \n",
    "            state = next_state\n",
    "\n",
    "        # Save checkpoints for later        \n",
    "        if global_time % SAVE_EVERY_X_STEPS == 0:            \n",
    "            saver.save(session, CHECKPOINT_PATH + '/network',\n",
    "                       global_step=tf.train.get_global_step())\n",
    "\n",
    "            # plot the results and save the figure            \n",
    "            plot_stats(stats)\n",
    "\n",
    "            fig_file = CHECKPOINT_PATH + '/stats.png'            \n",
    "            if os.path.isfile(fig_file):                \n",
    "                os.remove(fig_file)\n",
    "\n",
    "            plt.savefig(fig_file)            \n",
    "            plt.close()\n",
    "\n",
    "            # save the stats           \n",
    "            with open(CHECKPOINT_PATH + '/stats.arr', 'wb') as f:                \n",
    "                pickle.dump((stats.rewards, stats.lengths), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(stats):    \n",
    "    \n",
    "    \"\"\"Plot the stats\"\"\"    \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Episode\")\n",
    "    \n",
    "    # plot the rewards    \n",
    "    # rolling mean of 50    \n",
    "    cumsum = np.cumsum(np.insert(stats.rewards, 0, 0))    \n",
    "    rewards = (cumsum[50:] - cumsum[:-50]) / float(50)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    \n",
    "    ax1.set_ylabel('Reward', color=color)    \n",
    "    ax1.plot(rewards, color=color)    \n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # plot the episode lengths   \n",
    "    # rolling mean of 50   \n",
    "    cumsum = np.cumsum(np.insert(stats.lengths, 0, 0))   \n",
    "    lengths = (cumsum[50:] - cumsum[:-50]) / float(50)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'    \n",
    "    ax2.set_ylabel('Length', color=color)    \n",
    "    ax2.plot(lengths, color=color)    \n",
    "    ax2.tick_params(axis='y', labelcolor=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-77e12fb5cd45>:8: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-77e12fb5cd45>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Juan Esteban Cepeda\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-77e12fb5cd45>:22: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-77e12fb5cd45>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "\n",
      "Restoring checkpoint...\n",
      "INFO:tensorflow:Restoring parameters from deep_q_breakout_path_7\\network-20000\n",
      "Populating replay memory...\n",
      "Experience replay buffer: 100000 / 100000 initial (1000000 total)\n",
      "Copied model parameters to target network.\n",
      "Episode 1: time   211; reward 2.0; epsilon: 0.9818; loss: 0.007386; @ global step 20211 with total reward 2.0\n",
      "Episode 2: time   171; reward 1.0; epsilon: 0.9817; loss: 0.000045; @ global step 20382 with total reward 3.0\n",
      "Episode 3: time   193; reward 2.0; epsilon: 0.9815; loss: 0.000039; @ global step 20575 with total reward 5.0\n",
      "Episode 4: time   151; reward 0.0; epsilon: 0.9813; loss: 0.000117; @ global step 20726 with total reward 5.0\n",
      "Episode 5: time   168; reward 1.0; epsilon: 0.9812; loss: 0.000032; @ global step 20894 with total reward 6.0\n",
      "Episode 6: time   178; reward 1.0; epsilon: 0.9810; loss: 0.000027; @ global step 21072 with total reward 7.0\n",
      "Episode 7: time   147; reward 0.0; epsilon: 0.9809; loss: 0.000056; @ global step 21219 with total reward 7.0\n",
      "Episode 8: time   137; reward 0.0; epsilon: 0.9808; loss: 0.000116; @ global step 21356 with total reward 7.0\n",
      "Episode 9: time   158; reward 1.0; epsilon: 0.9806; loss: 0.000021; @ global step 21514 with total reward 8.0\n",
      "Episode 10: time   219; reward 2.0; epsilon: 0.9804; loss: 0.000007; @ global step 21733 with total reward 10.0\n",
      "Episode 11: time   151; reward 0.0; epsilon: 0.9803; loss: 0.000190; @ global step 21884 with total reward 10.0\n",
      "Episode 12: time   161; reward 0.0; epsilon: 0.9802; loss: 0.000079; @ global step 22045 with total reward 10.0\n",
      "Episode 13: time   171; reward 1.0; epsilon: 0.9800; loss: 0.000578; @ global step 22216 with total reward 11.0\n",
      "Episode 14: time   144; reward 0.0; epsilon: 0.9799; loss: 0.000052; @ global step 22360 with total reward 11.0\n",
      "Episode 15: time   146; reward 0.0; epsilon: 0.9797; loss: 0.000100; @ global step 22506 with total reward 11.0\n",
      "Episode 16: time   131; reward 0.0; epsilon: 0.9796; loss: 0.000213; @ global step 22637 with total reward 11.0\n",
      "Episode 17: time   129; reward 0.0; epsilon: 0.9795; loss: 0.000033; @ global step 22766 with total reward 11.0\n",
      "Episode 18: time   127; reward 0.0; epsilon: 0.9794; loss: 0.000092; @ global step 22893 with total reward 11.0\n",
      "Episode 19: time   221; reward 2.0; epsilon: 0.9792; loss: 0.000171; @ global step 23114 with total reward 13.0\n",
      "Episode 20: time   136; reward 0.0; epsilon: 0.9791; loss: 0.000024; @ global step 23250 with total reward 13.0\n",
      "Episode 21: time   134; reward 0.0; epsilon: 0.9790; loss: 0.000016; @ global step 23384 with total reward 13.0\n",
      "Episode 22: time   135; reward 0.0; epsilon: 0.9788; loss: 0.000057; @ global step 23519 with total reward 13.0\n",
      "Episode 23: time   158; reward 1.0; epsilon: 0.9787; loss: 0.000026; @ global step 23677 with total reward 14.0\n",
      "Episode 24: time   156; reward 0.0; epsilon: 0.9786; loss: 0.000123; @ global step 23833 with total reward 14.0\n",
      "Episode 25: time   255; reward 3.0; epsilon: 0.9783; loss: 0.000005; @ global step 24088 with total reward 17.0\n",
      "Episode 26: time   190; reward 2.0; epsilon: 0.9782; loss: 0.000014; @ global step 24278 with total reward 19.0"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    session, q_network, t_network, t_net_updates, frame_proc, saver, train_op, env = initialize()    \n",
    "    deep_q_learning() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
