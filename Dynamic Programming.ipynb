{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming Introduction.\n",
    "\n",
    "---\n",
    "\n",
    "I highly recommend to visit David Silver Lectures on Planning by Dynamic Programming: https://www.youtube.com/watch?v=Nd1-UUMVfz4\n",
    "\n",
    "**Dynamic Programming** is a very general solution method for problems which have two properties:\n",
    "1. Optimal substructure, i.e, Principle of optimality applies and optimal solution can be decomposed into subproblems. \n",
    "2. Overlapping subproblems, i.e, subproblems recur many times and solutions can be cached and reused. \n",
    "Fortunately, Markov decision processes satisfy both properties: Bellman equation gives recursive decomposition and value function stores a reuses solutions.\n",
    "\n",
    "The esence of dynamic programming is based on the fact that we have full knowledge of the MDP, which we will use for planning. But, what does full knowledge means? It means we can completely characterize all the states, actions, transition probabilities and reward function of our stochastic process (in this notebook, we will work with a deterministic process). This is an extremely weird case when developing IA in practice!\n",
    "\n",
    "In this section, we will solve the **Grid World Problem**. Hope you enjoy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Graphics module ready!\")\n",
    "        \n",
    "    def render(self, x, y, environment, plot_values = True):\n",
    "            \n",
    "        fig1 = plt.figure(figsize=(4, 4))\n",
    "        ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "\n",
    "        # Horizontal lines.\n",
    "        for i in range(0, 6):\n",
    "            ax1.axhline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "            ax1.axvline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "\n",
    "        # Salida, Meta & GameOver.\n",
    "        ax1.add_patch(patches.Rectangle((0.0, 0.0), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.8), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.2), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.6), 0.2, 0.2, facecolor = \"#68FF33\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.8, 0.8), 0.2, 0.2, facecolor = \"#FF5533\"))\n",
    "        \n",
    "        # Muros del juego.\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.4), 0.2, 0.4, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.6, 0.2), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.0), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.4), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        # Limit grid view.\n",
    "        plt.ylim((0, 1))\n",
    "        plt.xlim((0, 1))\n",
    "\n",
    "        # Plot player.\n",
    "        plt.scatter(x, y, s = 100, color = \"black\", marker = \"o\", facecolor = \"blue\", edgecolors = \"blue\", zorder = 10)\n",
    "\n",
    "        # Plot state values.\n",
    "        if plot_values:\n",
    "            for i in range(0, len(environment.value_state_table)):\n",
    "                for j in range(0, len(environment.value_state_table[0])):\n",
    "                    plt.text(environment.grid_pos[i] - 0.08, environment.grid_pos[j] - 0.03, \n",
    "                             round(environment.value_state_table[i][j], 2), fontsize=16)\n",
    "                \n",
    "        # Plot grid.\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.rw = -1 # Living (Movement) Penalty\n",
    "        self.walls_and_paths = [[1, 1, 1, 1, 1], [0, 1, 0, 0, 1], [1, 1, 1, 1, 0], [1, 0, 1, 1, 1], [1, 1, 0, 1, 1]]\n",
    "        self.rewards = [[self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw]]\n",
    "        self.grid_pos = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        self.value_state_table = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 100, 0]]\n",
    "        \n",
    "    def getStateValue(self, position):\n",
    "        return self.value_state_table[position[0]][position[1]]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.value_state_table = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 100, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphics module ready!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = GridEnvironment()\n",
    "graph = Graphics()\n",
    "graph.render(0.1, 0.1, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class valueBasedAgent():\n",
    "    \n",
    "    def __init__(self, environment, policy, discount_factor):\n",
    "        self.pos = [0,0]\n",
    "        self.total_reward = 0\n",
    "        self.environment = environment\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Start with a random policy. 0.25 chance of moving to any direction.\n",
    "        self.policy = policy   \n",
    "            \n",
    "    def forwardState(self, pos, action):\n",
    "        \n",
    "        # New position array.\n",
    "        new_position = pos\n",
    "        \n",
    "        # Compute new position based on action taken.\n",
    "        if(action == \"up\" and pos[1] < 4):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] + 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] + 1]\n",
    "\n",
    "        elif(action == \"down\" and pos[1] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] - 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] - 1]\n",
    "        elif(action == \"left\" and pos[0] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0] - 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] - 1, pos[1]]\n",
    "\n",
    "        elif(action == \"right\" and pos[0] < 4):\n",
    "            if(self.environment.walls_and_paths[pos[0] + 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] + 1, pos[1]]\n",
    "        return new_position\n",
    "        \n",
    "        \n",
    "    def valueFunction(self):\n",
    "            \n",
    "        # Initialize variable.\n",
    "        new_state_value = 0\n",
    "    \n",
    "        # Random movement!\n",
    "        if self.policy[self.pos[0]][self.pos[1]] == \"r\":\n",
    "            for action in self.actions:        \n",
    "                forward_state = self.forwardState(self.pos, action)\n",
    "                expected_return = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "                new_state_value += expected_return * 0.25\n",
    "            return new_state_value\n",
    "        \n",
    "        # Not random movement!\n",
    "        else: \n",
    "            action = self.policy[self.pos[0]][self.pos[1]]\n",
    "            forward_state = self.forwardState(self.pos, action)\n",
    "            expected_return = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "            new_state_value += expected_return\n",
    "            return new_state_value\n",
    "        \n",
    "    def getPosition(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def getReward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def setPosition(self, x, y):\n",
    "        self.pos = [x, y]\n",
    "        \n",
    "    def updateValueStateTable(self):\n",
    "        new_state_value = self.valueFunction()\n",
    "        self.environment.value_state_table[self.pos[0]][self.pos[1]] = new_state_value\n",
    "        \n",
    "    def selectBestAction(self):\n",
    "        \n",
    "        go_up = self.forwardState(self.pos, \"up\")\n",
    "        go_down = self.forwardState(self.pos, \"down\")\n",
    "        go_left = self.forwardState(self.pos, \"left\")\n",
    "        go_right = self.forwardState(self.pos, \"right\")\n",
    "        \n",
    "        up_value = self.environment.value_state_table[go_up[0]][go_up[1]]\n",
    "        down_value =  self.environment.value_state_table[go_down[0]][go_down[1]]\n",
    "        left_value =  self.environment.value_state_table[go_left[0]][go_left[1]]\n",
    "        right_value =  self.environment.value_state_table[go_right[0]][go_right[1]]\n",
    "        values = [up_value, down_value, left_value, right_value]\n",
    "        \n",
    "        best_action = self.actions[values.index(max(values))] \n",
    "        return best_action       \n",
    "            \n",
    "    def move(self):\n",
    "    \n",
    "        # Select action according to policy.\n",
    "        action = self.policy[self.pos[0]][self.pos[1]]\n",
    "        print(\"Action taken\", action)\n",
    "\n",
    "        # Move to new position according to action taken.\n",
    "        self.pos = self.forwardState(self.pos, action)\n",
    "        print(\"New Position: \", self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluation(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent, iterations = 3):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent                     \n",
    "        #print(\"GridWorld Initialize!\")\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def evaluate(self, plot_grid = True):\n",
    "        self.DP_policy_evaluation(self.iterations, plot_grid)\n",
    "        \n",
    "    def DP_policy_evaluation(self, iterations, plot_grid):\n",
    "        \n",
    "        for k in range(0, iterations):\n",
    "        \n",
    "            for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "\n",
    "                    if self.environment.walls_and_paths[i][j] == 1 and self.canChangeStateValue(i, j):\n",
    "\n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        self.agent.updateValueStateTable()\n",
    "\n",
    "                        # Method of the super class.\n",
    "                        if(plot_grid):\n",
    "                            \n",
    "                            # Render game.\n",
    "                            pos = self.agent.getPosition()\n",
    "                            grid_coords = self.environment.grid_pos\n",
    "                            \n",
    "                            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, True)\n",
    "                            time.sleep(0.01)\n",
    "                            clear_output(wait = True)\n",
    "                            \n",
    "    \n",
    "\n",
    "    def canChangeStateValue(self, x, y):\n",
    "        cant_modify = bool((x == 4 and y == 3)) # or (x == 4 and y == 4))\n",
    "        \n",
    "        grid = self.environment.walls_and_paths\n",
    "        coords = list()\n",
    "        \n",
    "        # Get walls.\n",
    "        for i in range(0, len(grid)):\n",
    "            for j in range(0, len(grid[0])):\n",
    "                if grid[i][j] == 0:\n",
    "                    coords.append([i, j])\n",
    "        for c in coords: \n",
    "            if c == [x, y]:\n",
    "                cant_modify = True\n",
    "                break\n",
    "                \n",
    "        return not cant_modify\n",
    "    \n",
    "    def updatePolicy(self):\n",
    "        \n",
    "         for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "                    if self.environment.walls_and_paths[i][j] == 1:\n",
    "                        \n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        best_action = self.agent.selectBestAction()\n",
    "                        self.agent.policy[i][j] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent             \n",
    "        print(\"GridWorld Initialize!\")\n",
    "                \n",
    "    def update(self, secs):\n",
    "        \n",
    "        pos = self.agent.getPosition()\n",
    "        grid_coords = self.environment.grid_pos\n",
    "        self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        while not ((self.agent.pos[0] == 4 and self.agent.pos[1] == 4) or (self.agent.pos[0] == 4 and self.agent.pos[1] == 3)):\n",
    "            \n",
    "            self.agent.move()\n",
    "            pos = self.agent.getPosition()\n",
    "            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "            \n",
    "            time.sleep(secs)\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "        #self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        #time.sleep(secs)\n",
    "        #print(\"Yuhuu, we won the game!\")\n",
    "        #clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Evaluation (Prediction or Planning) for DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "\n",
    "# Initialize environment and agent.\n",
    "discount_factor = 0.5\n",
    "environment = GridEnvironment()\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Initialize policy evaluation class.\n",
    "policy_evaluation = PolicyEvaluation(environment, agent, iterations = 1000)\n",
    "policy_evaluation.evaluate(plot_grid = False)\n",
    "policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'right'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New agent policy after policy evaluation.\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win the Game with the previous policy evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken right\n",
      "New Position:  [4, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [1, 4]\n",
    "game = Game(environment, agent)\n",
    "game.update(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken right\n",
      "New Position:  [4, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAFQ0lEQVR4nO3bsWuTeRzH8W+0KViK/QMEF/EGh7gEBA9XQTehgqC3uh8O/RsqyC2ObqcgWHBTcD9ByGIW4cRF8A8oFoVG77mpyJ3Pk4jXmE+412t8fk/Jl5B3vmlpek3TFJDnyKIHANqJE0KJE0KJE0KJE0KtTDscDIb+lAtzNh6Pem3Xp8Y57QfTHLyRvNjZXfQoM53b3KiqqtWt0YIn+Tb728OqWo7XwsHrYBlmrZq+AH2shVDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFC9pmk6DweDYfchcCjG41Gv7brNCaFWZt3QVXWagy2/ujVa9Cgz7W8Pq2r5nttlmPdg1p/Hi57k2/wx6D6zOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCHUzO9zwv/Z7psT9fLO9Xp9/3JN9taqv/6hTt94UmdvPaiNU+/m+tg2J3R4+/R8PRo8rFf3rtTk/XpVc6Qm79fr1b0r9WjwsN4+PT/XxxcntNh9c6KebW7Xpw/Hqpn0/3HWTPr16cOxera5XbtvTsxtBnFCi5d3rtfnyfTf+j5PVmr82/W5zSBOaPH6/uWvNua/NZN+/fn75bnNIE5oMdlbO9T7voc4oUV//cOh3vc9xAktTt94Ur3+ZOo9vf6kfvrlydxmECe0OHvrQR3tf5p6z9H+pxr8+mBuM4gTWmycelcXd7ZqZe3jVxu015/UytrHurizNdd/RBAndDh56XldHV+rMzcfV//4XtWRv6p/fK/O3HxcV8fX6uSl53N9fP++B1NsnHpXF+7ergt3b//wx7Y5IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IVSvaZrOw8Fg2H0IHIrxeNRru25zQqiZ3+fsqjrNwZZf3RotepSZ9reHVbV8z+0yzLtMr4OqL6+FNjYnhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhOo1TdN5OBgMuw+BQzEej3pt121OCLUy64auqtMcbPllmHeZZq36Mu/q1mjRo8y0vz2sqqoXO7sLnuTbnNvc6DyzOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCFUr2mazsPBYNh9CByK8XjUa7tuc0KolVk3vNjZ/RFz/GfnNjeqqmp1a7TgSWbb3x5WVfc7ZpqDT1DLMO8yzVo1/dOpzQmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhek3TdB4OBsPuQ+BQjMejXtv1qXECi+NjLYQSJ4QSJ4QSJ4QSJ4QSJ4T6G5ym55mNyI60AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [4, 1]\n",
    "game = Game(environment, agent)\n",
    "game.update(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken right\n",
      "New Position:  [4, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [0, 0]\n",
    "game = Game(environment, agent)\n",
    "game.update(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We made it! We have developed our first artificial intelligence. As you can see, we took a random policy and evaluate it through our policity evaluation algorithm, then, we encourage our agent to take the best action for each state, which lead us to an amazing result.\n",
    "\n",
    "Something that surprised me the very first time I learnt about **Dynamic Programming**, was the fact that even though we chose a random policy, it led us to a solution of the grid world problem. According to the this, how our solution could change if we would have choosen a more reasonable policy? This is the question that we are going to try to answer in the following section, through **Policity Iteration** and **Value Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Iteration (Control). Improving our Policy.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2624/1*udhphWhqjadT-osAQhL6AQ.png\" width =\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "    \n",
    "# Initaliza environment and agent.\n",
    "discount_factor = 0.5\n",
    "environment = GridEnvironment()\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Policy iteration algorithm.\n",
    "for i in range(0, 1000):\n",
    "\n",
    "    # Reset value function.\n",
    "    environment.reset()\n",
    "\n",
    "    # Evaluate new policy.\n",
    "    policy_evaluation = PolicyEvaluation(environment, agent, iterations = 10)\n",
    "    policy_evaluation.evaluate(plot_grid = False)\n",
    "    policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'down'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Value Iteration (Control). Improving our Policy.\n",
    "\n",
    "\n",
    "<img src=\"https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-319-63387-9_10/MediaObjects/454766_1_En_10_Figa_HTML.gif\" width =\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "    \n",
    "# Initaliza environment and agent.\n",
    "discount_factor = 0.6\n",
    "environment = GridEnvironment()\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Policy iteration algorithm.\n",
    "for i in range(0, 1000):\n",
    "\n",
    "    # Reset value function.\n",
    "    # environment.reset() => We do not reset the environment? \n",
    "\n",
    "    # Evaluate new policy.\n",
    "    policy_evaluation = PolicyEvaluation(environment, agent, iterations = 1)\n",
    "    policy_evaluation.evaluate(plot_grid = False)\n",
    "    policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'down'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it doesn't matter which method we use, it is guarantee by the Contraction Mapping Theorem to always converge to the optimal policy. In future lectures, we will see that it is not always the case when we can compute the optimal policy, that's why we will present new algorithms that allow us to aproximmate our value function, and therefore the optimal policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
