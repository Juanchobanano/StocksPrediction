{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dynamic Programming Introduction.\n",
    "\n",
    "---\n",
    "* **Author**: Juan Esteban Cepeda Baena.\n",
    "* **Linkedin**: https://www.linkedin.com/in/juan-e-cepeda-gestion/\n",
    "* **Email**: juancepeda.gestion@gmail.com / jecepedab@unal.edu.co\n",
    "* **Google Site**: https://sites.google.com/view/juancepeda/\n",
    "* **Code License**: NonCommercial. **If you are going to use my code, please put a link to my github's profile.**\n",
    "---\n",
    "\n",
    "Welcome to the first module of my little course on Reinforcement Learning! As I mentioned in the course presentation notebook, we will spent some time styding a brief introduction on Dynamic Programming. **Dynamyc Programming** is a very general solution method for problems which have two properties:\n",
    "\n",
    "1. Optimal substructure, i.e, Principle of optimality applies and optimal solution can be decomposed into subproblems. \n",
    "\n",
    "2. Overlapping subproblems, i.e, subproblems recur many times and solutions can be cached and reused. \n",
    "\n",
    "Fortunately, a markov decision process (or MDP for sake of simplicity) satisfy both properties: Bellman equation gives recursive decomposition and value function stores a reuses solutions. The esence of dynamic programming is based on the fact that **we have full knowledge of the MDP, which we will use for planning the solution of a given problem**. But, what does full knowledge means? It means we can completely characterize all the states, actions, transition probabilities and reward function of our stochastic process. This is an extremely weird case when developing IA in practice, since we don't usually know the reward function nor the transition probability matrix.\n",
    "\n",
    "Before moving forward, I highly recommend to visit David Silver Lectures on Planning by Dynamic Programming: https://www.youtube.com/watch?v=Nd1-UUMVfz4, which will help you to understand what we are going to do next.\n",
    "\n",
    "In this section, we will solve the **Grid World Problem**, which consist on several elements: a labyrinth of size $n^2$. For accomplish this task, we simplified some elements of the MDP process. \n",
    "\n",
    "Instead of using the common Bellman's equations,\n",
    "1. $V_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} (P_{ss'}^{a}[R_{ss'}^{a} + \\gamma V_{\\pi}(s')])$ (planning)\n",
    "\n",
    "2. $\\pi'(s) = {argmax}_{a} \\sum_{s'} (P_{ss'}^{a}[R_{ss'}^{a} + \\gamma V_{\\pi}(s')]$ (control).\n",
    "\n",
    "we use, \n",
    "1. $V_{\\pi}(s) = \\sum_{a} \\pi(a|s) [R_{ss_{a}} + \\gamma V_{\\pi}(s_{a})]$ (planning)\n",
    "2. $\\pi'(s) = \\max_{a}(R_{ss_{a}} + \\gamma V_{\\pi}(s_{a}))$ (control).\n",
    "\n",
    "respectively.\n",
    "As you can see, we do not include a transition probability function nor a probabilistic policy, which means we are working with a **deterministic policy and environment.**\n",
    "\n",
    "Hope you enjoy it!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"./imagenes/dynamic_programming_backup.png\" width = \"500\" height = \"400\">\n",
    "<center>Image 1. Dynamic Programming Planning Process</center>\n",
    "\n",
    "\n",
    "I highly recommend to visit David Silver Lectures on Planning by Dynamic Programming: https://www.youtube.com/watch?v=Nd1-UUMVfz4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will solve the **Grid World Problem**. For accomplish this task, we simplified some elements of the MDP process. \n",
    "\n",
    "Instead of using the common Bellman's equations,\n",
    "1. $V_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} (P_{ss'}^{a}[R_{ss'}^{a} + \\gamma V_{\\pi}(s')])$ (planning)\n",
    "\n",
    "2. $\\pi'(s) = {argmax}_{a} \\sum_{s'} (P_{ss'}^{a}[R_{ss'}^{a} + \\gamma V_{\\pi}(s')]$ (control).\n",
    "\n",
    "we use, \n",
    "1. $V_{\\pi}(s) = \\sum_{a} \\pi(a|s) [R_{ss_{a}} + \\gamma V_{\\pi}(s_{a})]$ (planning)\n",
    "2. $\\pi'(s) = \\max_{a}(R_{ss_{a}} + \\gamma V_{\\pi}(s_{a}))$ (control).\n",
    "\n",
    "respectively.\n",
    "As you can see, we do not include a transition probability function nor a probabilistic policy, which means we are working with a **deterministic policy and environment.**\n",
    "\n",
    "Hope you enjoy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Graphics module ready!\")\n",
    "        \n",
    "    def render(self, x, y, environment, plot_values = True):\n",
    "            \n",
    "        fig1 = plt.figure(figsize=(4, 4))\n",
    "        ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "\n",
    "        # Horizontal lines.\n",
    "        for i in range(0, 6):\n",
    "            ax1.axhline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "            ax1.axvline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "\n",
    "        # Salida, Meta & GameOver.\n",
    "        ax1.add_patch(patches.Rectangle((0.0, 0.0), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.8), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.2), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.6), 0.2, 0.2, facecolor = \"#68FF33\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.8, 0.8), 0.2, 0.2, facecolor = \"#FF5533\"))\n",
    "        \n",
    "        # Muros del juego.\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.4), 0.2, 0.4, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.6, 0.2), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.0), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.4), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        # Limit grid view.\n",
    "        plt.ylim((0, 1))\n",
    "        plt.xlim((0, 1))\n",
    "\n",
    "        # Plot player.\n",
    "        plt.scatter(x, y, s = 100, color = \"black\", marker = \"o\", facecolor = \"blue\", edgecolors = \"blue\", zorder = 10)\n",
    "\n",
    "        # Plot state values.\n",
    "        if plot_values:\n",
    "            for i in range(0, len(environment.value_state_table)):\n",
    "                for j in range(0, len(environment.value_state_table[0])):\n",
    "                    plt.text(environment.grid_pos[i] - 0.08, environment.grid_pos[j] - 0.03, \n",
    "                             round(environment.value_state_table[i][j], 1), fontsize=16)\n",
    "                \n",
    "        # Plot grid.\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.rw = -1 # Living (Movement) Penalty\n",
    "        self.walls_and_paths = [[1, 1, 1, 1, 1], [0, 1, 0, 0, 1], [1, 1, 1, 1, 0], [1, 0, 1, 1, 1], [1, 1, 0, 1, 1]]\n",
    "        self.rewards = [[self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw]]\n",
    "        self.grid_pos = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        self.value_state_table = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 100, 0]]\n",
    "    \n",
    "    def getStateValue(self, position):\n",
    "        return self.value_state_table[position[0]][position[1]]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.value_state_table = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 100, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphics module ready!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = GridEnvironment()\n",
    "graph = Graphics()\n",
    "graph.render(0.1, 0.1, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class valueBasedAgent():\n",
    "    \n",
    "    def __init__(self, environment, policy, discount_factor):\n",
    "        self.pos = [0,0]\n",
    "        self.total_reward = 0\n",
    "        self.environment = environment\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Start with a random policy. 0.25 chance of moving to any direction.\n",
    "        self.policy = policy   \n",
    "            \n",
    "    def forwardState(self, pos, action):\n",
    "        \n",
    "        # New position array.\n",
    "        new_position = pos\n",
    "        \n",
    "        # Compute new position based on action taken.\n",
    "        if(action == \"up\" and pos[1] < 4):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] + 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] + 1]\n",
    "\n",
    "        elif(action == \"down\" and pos[1] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] - 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] - 1]\n",
    "                \n",
    "        elif(action == \"left\" and pos[0] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0] - 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] - 1, pos[1]]\n",
    "\n",
    "        elif(action == \"right\" and pos[0] < 4):\n",
    "            if(self.environment.walls_and_paths[pos[0] + 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] + 1, pos[1]]\n",
    "        return new_position\n",
    "        \n",
    "        \n",
    "    def valueFunction(self):\n",
    "            \n",
    "        # Initialize variable.\n",
    "        new_state_value = 0\n",
    "    \n",
    "        # Random movement!\n",
    "        if self.policy[self.pos[0]][self.pos[1]] == \"r\":\n",
    "            for action in self.actions:        \n",
    "                forward_state = self.forwardState(self.pos, action)\n",
    "                \n",
    "                # Simplified version of Q-value.\n",
    "                q_value = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "                new_state_value += q_value * 0.25\n",
    "            return new_state_value\n",
    "        \n",
    "        # Not random movement!\n",
    "        else: \n",
    "            action = self.policy[self.pos[0]][self.pos[1]]\n",
    "            forward_state = self.forwardState(self.pos, action)\n",
    "            \n",
    "            # Simplified version of Q-value.\n",
    "            q_value = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "            new_state_value += q_value\n",
    "            return new_state_value\n",
    "        \n",
    "    def getPosition(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def getReward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def setPosition(self, x, y):\n",
    "        self.pos = [x, y]\n",
    "        \n",
    "    def updateValueStateTable(self):\n",
    "        new_state_value = self.valueFunction()\n",
    "        self.environment.value_state_table[self.pos[0]][self.pos[1]] = new_state_value\n",
    "        \n",
    "    def selectBestAction(self):\n",
    "        \n",
    "        # Compute new possible states.\n",
    "        go_up = self.forwardState(self.pos, \"up\")\n",
    "        go_down = self.forwardState(self.pos, \"down\")\n",
    "        go_left = self.forwardState(self.pos, \"left\")\n",
    "        go_right = self.forwardState(self.pos, \"right\")\n",
    "        \n",
    "        # Q values (simplified version).\n",
    "        up_value = (self.environment.rewards[go_up[0]][go_up[1]] + \n",
    "                    self.discount_factor * self.environment.value_state_table[go_up[0]][go_up[1]])\n",
    "        down_value = (self.environment.rewards[go_down[0]][go_down[1]] + \n",
    "                      self.discount_factor * self.environment.value_state_table[go_down[0]][go_down[1]])\n",
    "        left_value = (self.environment.rewards[go_left[0]][go_left[1]] + \n",
    "                        self.discount_factor * self.environment.value_state_table[go_left[0]][go_left[1]])\n",
    "        right_value = (self.environment.rewards[go_right[0]][go_right[1]] + \n",
    "                       self.discount_factor * self.environment.value_state_table[go_right[0]][go_right[1]])\n",
    "        \n",
    "        # Array of Q-values.\n",
    "        values = [up_value, down_value, left_value, right_value]\n",
    "        \n",
    "        best_action = self.actions[values.index(max(values))] \n",
    "        return best_action       \n",
    "            \n",
    "    def move(self):\n",
    "    \n",
    "        # Select action according to policy.\n",
    "        action = self.policy[self.pos[0]][self.pos[1]]\n",
    "        print(\"Action taken\", action)\n",
    "\n",
    "        # Move to new position according to action taken.\n",
    "        self.pos = self.forwardState(self.pos, action)\n",
    "        print(\"New Position: \", self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluation(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent, iterations = 3):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent                     \n",
    "        #print(\"GridWorld Initialize!\")\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def evaluate(self, plot_grid = True):\n",
    "        self.DP_policy_evaluation(self.iterations, plot_grid)\n",
    "        \n",
    "    def DP_policy_evaluation(self, iterations, plot_grid):\n",
    "        \n",
    "        for k in range(0, iterations):\n",
    "            for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "\n",
    "                    if self.environment.walls_and_paths[i][j] == 1 and self.canChangeStateValue(i, j):\n",
    "\n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        self.agent.updateValueStateTable()\n",
    "\n",
    "                        # Method of the super class.\n",
    "                        if(plot_grid):\n",
    "                            \n",
    "                            # Render game.\n",
    "                            pos = self.agent.getPosition()\n",
    "                            grid_coords = self.environment.grid_pos\n",
    "                            \n",
    "                            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, True)\n",
    "                            time.sleep(0.01)\n",
    "                            clear_output(wait = True)\n",
    "                            \n",
    "    \n",
    "\n",
    "    def canChangeStateValue(self, x, y):\n",
    "        cant_modify = bool((x == 4 and y == 3)) # or (x == 4 and y == 4))\n",
    "        \n",
    "        grid = self.environment.walls_and_paths\n",
    "        coords = list()\n",
    "        \n",
    "        # Get walls.\n",
    "        for i in range(0, len(grid)):\n",
    "            for j in range(0, len(grid[0])):\n",
    "                if grid[i][j] == 0:\n",
    "                    coords.append([i, j])\n",
    "        for c in coords: \n",
    "            if c == [x, y]:\n",
    "                cant_modify = True\n",
    "                break\n",
    "                \n",
    "        return not cant_modify\n",
    "    \n",
    "    def updatePolicy(self):\n",
    "        \n",
    "         for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "                    if self.environment.walls_and_paths[i][j] == 1:\n",
    "                        \n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        best_action = self.agent.selectBestAction()\n",
    "                        self.agent.policy[i][j] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent             \n",
    "        print(\"GridWorld Initialize!\")\n",
    "                \n",
    "    def update(self, secs):\n",
    "        \n",
    "        pos = self.agent.getPosition()\n",
    "        grid_coords = self.environment.grid_pos\n",
    "        self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        while not ((self.agent.pos[0] == 4 and self.agent.pos[1] == 4) or (self.agent.pos[0] == 4 and self.agent.pos[1] == 3)):\n",
    "            \n",
    "            self.agent.move()\n",
    "            pos = self.agent.getPosition()\n",
    "            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "            \n",
    "            time.sleep(secs)\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "        #self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        #time.sleep(secs)\n",
    "        #print(\"Yuhuu, we won the game!\")\n",
    "        #clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Evaluation (Planning) for DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "\n",
    "# Initialize environment and agent.\n",
    "discount_factor = 1\n",
    "environment = GridEnvironment()\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Initialize policy evaluation class.\n",
    "policy_evaluation = PolicyEvaluation(environment, agent, iterations = 1)\n",
    "policy_evaluation.evaluate(plot_grid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-110.87741692908136,\n",
       "  -106.88071433925548,\n",
       "  -122.86911267445035,\n",
       "  -134.8602296212664,\n",
       "  -142.8542261090061],\n",
       " [0, -82.8979977735338, 0, 0, -146.85120389131987],\n",
       " [-70.90855702053615,\n",
       "  -54.917222827805965,\n",
       "  -6.946039455371832,\n",
       "  19.039104809861588,\n",
       "  0],\n",
       " [-82.90235097388641,\n",
       "  0,\n",
       "  19.039104809861584,\n",
       "  49.0235600559263,\n",
       "  62.01592280060383],\n",
       " [-90.89815668658741, -94.89604524769868, 0, 100, 79.00801566941342]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.value_state_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'right'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New agent policy after policy evaluation.\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win the Game with the previous policy evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken right\n",
      "New Position:  [4, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAFQ0lEQVR4nO3bsWuTeRzH8W+0KViK/QMEF/EGh7gEBA9XQTehgqC3uh8O/RsqyC2ObqcgWHBTcD9ByGIW4cRF8A8oFoVG77mpyJ3Pk4jXmE+412t8fk/Jl5B3vmlpek3TFJDnyKIHANqJE0KJE0KJE0KJE0KtTDscDIb+lAtzNh6Pem3Xp8Y57QfTHLyRvNjZXfQoM53b3KiqqtWt0YIn+Tb728OqWo7XwsHrYBlmrZq+AH2shVDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFC9pmk6DweDYfchcCjG41Gv7brNCaFWZt3QVXWagy2/ujVa9Cgz7W8Pq2r5nttlmPdg1p/Hi57k2/wx6D6zOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCHUzO9zwv/Z7psT9fLO9Xp9/3JN9taqv/6hTt94UmdvPaiNU+/m+tg2J3R4+/R8PRo8rFf3rtTk/XpVc6Qm79fr1b0r9WjwsN4+PT/XxxcntNh9c6KebW7Xpw/Hqpn0/3HWTPr16cOxera5XbtvTsxtBnFCi5d3rtfnyfTf+j5PVmr82/W5zSBOaPH6/uWvNua/NZN+/fn75bnNIE5oMdlbO9T7voc4oUV//cOh3vc9xAktTt94Ur3+ZOo9vf6kfvrlydxmECe0OHvrQR3tf5p6z9H+pxr8+mBuM4gTWmycelcXd7ZqZe3jVxu015/UytrHurizNdd/RBAndDh56XldHV+rMzcfV//4XtWRv6p/fK/O3HxcV8fX6uSl53N9fP++B1NsnHpXF+7ergt3b//wx7Y5IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IVSvaZrOw8Fg2H0IHIrxeNRru25zQqiZ3+fsqjrNwZZf3RotepSZ9reHVbV8z+0yzLtMr4OqL6+FNjYnhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhOo1TdN5OBgMuw+BQzEej3pt121OCLUy64auqtMcbPllmHeZZq36Mu/q1mjRo8y0vz2sqqoXO7sLnuTbnNvc6DyzOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCFUr2mazsPBYNh9CByK8XjUa7tuc0KolVk3vNjZ/RFz/GfnNjeqqmp1a7TgSWbb3x5WVfc7ZpqDT1DLMO8yzVo1/dOpzQmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhxAmhek3TdB4OBsPuQ+BQjMejXtv1qXECi+NjLYQSJ4QSJ4QSJ4QSJ4QSJ4T6G5ym55mNyI60AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [1, 4]\n",
    "game = Game(environment, agent)\n",
    "game.update(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken right\n",
      "New Position:  [4, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [4, 1]\n",
    "game = Game(environment, agent)\n",
    "game.update(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken right\n",
      "New Position:  [4, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [0, 0]\n",
    "game = Game(environment, agent)\n",
    "game.update(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We made it! We have developed our first artificial intelligence. As you can see, we took a random policy and evaluate it through our policity evaluation algorithm, then, we encourage our agent to take the best action for each state, which lead us to an amazing result.\n",
    "\n",
    "Something that surprised me the very first time I learnt about **Dynamic Programming**, was the fact that even though we chose a random policy, it led us to a solution of the grid world problem. According to the this, how our solution could change if we would have choosen a more reasonable policy? This is the question that we are going to try to answer in the following section, through **Policity Iteration** and **Value Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Iteration (Control). Improving our Policy.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2624/1*udhphWhqjadT-osAQhL6AQ.png\" width =\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "    \n",
    "# Initaliza environment and agent.\n",
    "discount_factor = 0.5\n",
    "environment = GridEnvironment()\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Policy iteration algorithm.\n",
    "for i in range(0, 1000):\n",
    "\n",
    "    # Reset value function.\n",
    "    environment.reset()\n",
    "\n",
    "    # Evaluate new policy.\n",
    "    policy_evaluation = PolicyEvaluation(environment, agent, iterations = 10)\n",
    "    policy_evaluation.evaluate(plot_grid = False)\n",
    "    policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'down'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Value Iteration (Control). Improving our Policy.\n",
    "\n",
    "\n",
    "<img src=\"https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-319-63387-9_10/MediaObjects/454766_1_En_10_Figa_HTML.gif\" width =\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "    \n",
    "# Initaliza environment and agent.\n",
    "discount_factor = 0.6\n",
    "environment = GridEnvironment()\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Policy iteration algorithm.\n",
    "for i in range(0, 1000):\n",
    "\n",
    "    # Reset value function.\n",
    "    # environment.reset() => We do not reset the environment? \n",
    "\n",
    "    # Evaluate new policy.\n",
    "    policy_evaluation = PolicyEvaluation(environment, agent, iterations = 1)\n",
    "    policy_evaluation.evaluate(plot_grid = False)\n",
    "    policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'right', 'r'],\n",
       " ['left', 'r', 'up', 'right', 'down'],\n",
       " ['left', 'down', 'r', 'down', 'down']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it doesn't matter which method we use, it is guarantee by the Contraction Mapping Theorem to always converge to the optimal policy. In future lectures, we will see that it is not always the case when we can compute the optimal policy, that's why we will present new algorithms that allow us to aproximmate our value function, and therefore the optimal policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
