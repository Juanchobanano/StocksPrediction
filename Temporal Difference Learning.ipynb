{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning Introduction.\n",
    "\n",
    "---\n",
    "\n",
    "I highly recommend to visit the David Silver Lectures on Temporal Difference Reinforcement Learning: https://www.youtube.com/watch?v=PnHCvfgC_ZA&t\n",
    "\n",
    "\n",
    "The Temporal Difference is class of model-free reinforcement learning method. Even though these methods learn directly from experiencie (trial and error) as explained in the MonteCarlo section, its difference with MonteCarlo is based on the fact that they don't have to wait until the end of an episode. Instead, they can update the action-value function after each step of the episode. In its most basic form, a TD algorithm uses the following formula to perform a state-value update:\n",
    "\n",
    "$V(S_{t}) = V(S_{t}) + \\alpha [r_{t+1} + \\gamma V(S_{t+1}) - V{S_{t}}]$\n",
    "\n",
    "Where $\\alpha$ is called step size (learning rate) and it's in the range of $[0, 1]$.\n",
    "\n",
    "In this notebook, we first consider the problem of the policy evaluation using temporal difference learning, then, we continue with policy control.\n",
    "\n",
    "<img src = \"./imagenes/temporal_difference_backup.png\" width = \"500px\" height = \"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Evaluation (Prediction) for Temporal Difference Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./imagenes/temporal_difference_policy_evaluation_algorithm.png\" width = \"650px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Graphics module ready!\")\n",
    "        \n",
    "    def render(self, x, y, policy_evaluator, plot_values = True):\n",
    "            \n",
    "        fig1 = plt.figure(figsize=(4, 4))\n",
    "        ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "\n",
    "        # Horizontal lines.\n",
    "        for i in range(0, 6):\n",
    "            ax1.axhline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "            ax1.axvline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "\n",
    "         # Salida, Meta & GameOver.\n",
    "        ax1.add_patch(patches.Rectangle((0.0, 0.0), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.8), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.2), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.6), 0.2, 0.2, facecolor = \"#68FF33\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.8, 0.8), 0.2, 0.2, facecolor = \"#FF5533\"))\n",
    "        \n",
    "        # Muros del juego.\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.4), 0.2, 0.4, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.6, 0.2), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.0), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.4), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        # Limit grid view.\n",
    "        plt.ylim((0, 1))\n",
    "        plt.xlim((0, 1))\n",
    "\n",
    "        # Plot player.\n",
    "        plt.scatter(x, y, s = 100, color = \"black\", marker = \"o\", facecolor = \"blue\", edgecolors = \"blue\", zorder = 10)\n",
    "\n",
    "        # Plot state values.\n",
    "        if plot_values:\n",
    "            for i in range(0, len(policy_evaluator.value_state_table)):\n",
    "                for j in range(0, len(policy_evaluator.value_state_table[0])):\n",
    "                    plt.text(self.environment.grid_pos[i] - 0.08, self.environment.grid_pos[j] - 0.03, \n",
    "                             round(policy_evaluator.value_state_table[i][j], 1), fontsize=10)\n",
    "                \n",
    "        # Plot grid.\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.rw = -1 # Living (Movement) Penalty\n",
    "        self.walls_and_paths = [[1, 1, 1, 1, 1], [0, 1, 0, 0, 1], [1, 1, 1, 1, 0], [1, 0, 1, 1, 1], [1, 1, 0, 1, 1]]\n",
    "        self.rewards = [[self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw], \n",
    "                        [self.rw, self.rw, self.rw, self.rw, self.rw]]\n",
    "        self.grid_pos = [0.1, 0.3, 0.5, 0.7, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelFreeAgent():\n",
    "    \n",
    "    def __init__(self, policy, discount_factor):\n",
    "        self.pos = [0,0]\n",
    "        self.total_reward = 0\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Start with a random policy. 0.25 chance of moving to any direction.\n",
    "        self.policy = policy   \n",
    "        \n",
    "    # Move!\n",
    "    def move(self, environment):\n",
    "    \n",
    "        # Select action according to policy.\n",
    "        action = self.policy[self.pos[0]][self.pos[1]]\n",
    "        if action == \"r\":\n",
    "            action = self.actions[random.randint(0, 3)]\n",
    "\n",
    "        # Move to new position according to action taken.\n",
    "        self.pos = self.forwardState(self.pos, action, environment)\n",
    "        \n",
    "    # Select action according to policy.\n",
    "    def selectAction(self, state):\n",
    "        \n",
    "        if(self.policy[state[0]][state[1]] == \"r\"):\n",
    "            action = self.actions[random.randint(0, len(self.available_actions) - 1)] # Agent initial policy.\n",
    "        else: \n",
    "            action = self.policy[state[0]][state[1]]\n",
    "        return action\n",
    "            \n",
    "    def forwardState(self, pos, action, environment):\n",
    "        \n",
    "        # New position array.\n",
    "        new_position = pos\n",
    "        \n",
    "        # Compute new position based on action taken.\n",
    "        if(action == \"up\" and pos[1] < 4):\n",
    "            if(environment.walls_and_paths[pos[0]][pos[1] + 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] + 1]\n",
    "        elif(action == \"down\" and pos[1] > 0):\n",
    "            if(environment.walls_and_paths[pos[0]][pos[1] - 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] - 1]\n",
    "        elif(action == \"left\" and pos[0] > 0):\n",
    "            if(environment.walls_and_paths[pos[0] - 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] - 1, pos[1]]\n",
    "        elif(action == \"right\" and pos[0] < 4):\n",
    "            if(environment.walls_and_paths[pos[0] + 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] + 1, pos[1]]\n",
    "        return new_position\n",
    "\n",
    "    def getPosition(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def getReward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def setPosition(self, x, y):\n",
    "        self.pos = [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluation(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent, ephocs = 10, learning_rate = 0.5, steps = 0):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent                     \n",
    "        self.ephocs = ephocs\n",
    "        self.value_state_table = [[0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 0, 0], \n",
    "                                  [0, 0, 0, 100, 0]]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = 0\n",
    "    \n",
    "    def evaluate(self, plot_grid = True):\n",
    "        self.updateValueStateTable_TemporalDifference(plot_grid, self.ephocs, self.steps)\n",
    "        \n",
    "    def updateValueStateTable_TemporalDifference(self, plot_grid, ephocs, n = 0):\n",
    "        \n",
    "        # Set agent position in the initial state.\n",
    "        self.agent.setPosition(0, 0)\n",
    "        \n",
    "        # Generate k epochs.\n",
    "        for k in range(0, ephocs):\n",
    "            \n",
    "            # Continue until get to the terminal state.\n",
    "            while not (self.agent.pos[0] == 4 and self.agent.pos[1] == 3):\n",
    "                \n",
    "                # Previous position.\n",
    "                pre = self.agent.pos.copy()\n",
    "                \n",
    "                # Move the player.\n",
    "                self.agent.move(self.environment)\n",
    "                \n",
    "                # Compute TD target.\n",
    "                reward = self.environment.rewards[self.agent.pos[0]][self.agent.pos[1]]\n",
    "                new_state_value = self.value_state_table[self.agent.pos[0]][self.agent.pos[1]]\n",
    "                TD_target = reward + self.agent.discount_factor * new_state_value\n",
    "                \n",
    "                # Compute TD error.\n",
    "                TD_error = TD_target - self.value_state_table[pre[0]][pre[1]]\n",
    "                \n",
    "                # Update value state table.\n",
    "                self.value_state_table[pre[0]][pre[1]] = (self.value_state_table[pre[0]][pre[1]] +\n",
    "                                                         self.learning_rate * (TD_error))\n",
    "                \n",
    "                # Method of the super class.\n",
    "                if(plot_grid):\n",
    "\n",
    "                    # Render game.\n",
    "                    pos = self.agent.getPosition()\n",
    "                    grid_coords = self.environment.grid_pos\n",
    "\n",
    "                    self.render(self.environment.grid_pos[pos[0]], self.environment.grid_pos[pos[1]], self, True)\n",
    "                    time.sleep(0.1)\n",
    "                    clear_output(wait = True)\n",
    "                    \n",
    "            # Set players position at the beginning.\n",
    "            self.agent.setPosition(0, 0)\n",
    "            \n",
    "    def plotTable(self):\n",
    "        # Render game.\n",
    "        grid_coords = self.environment.grid_pos\n",
    "        self.render(self.environment.grid_pos[0], self.environment.grid_pos[0], self, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Graphics):\n",
    "    \n",
    "    def __init__(self, environment, agent):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent             \n",
    "        \n",
    "    def update(self, secs):\n",
    "        \n",
    "        pos = self.agent.getPosition()\n",
    "        grid_coords = self.environment.grid_pos\n",
    "        self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        while not (self.agent.pos[0] == 4 and self.agent.pos[1] == 3):\n",
    "            \n",
    "            self.agent.move(self.environment)\n",
    "            pos = self.agent.getPosition()\n",
    "            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "            \n",
    "            time.sleep(secs)\n",
    "            clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Policy Evaluation - Temporal Difference Learning, TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "\n",
    "# Initialize environment and agent.\n",
    "discount_factor = 0.9\n",
    "environment = GridEnvironment()\n",
    "agent = modelFreeAgent(policy, discount_factor)\n",
    "\n",
    "# Initialize policy evaluation class.\n",
    "ephocs = 10000\n",
    "learning_rate = 0.5\n",
    "steps = 0\n",
    "policy_evaluation = PolicyEvaluation(environment, agent, ephocs, learning_rate, steps)\n",
    "policy_evaluation.evaluate(plot_grid = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot policy evaluation result.\n",
    "policy_evaluation.plotTable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(n).\n",
    "\n",
    "---\n",
    "\n",
    "In the previous example, we updated our value function table after observing what happened one step in the future, however, we could have done the same after considering $n$-steps in the future, i.e, we can generalize this process as indicated in the image (we will incorporate this functionality in future versions of this notebook):\n",
    "\n",
    "<img src = \"./imagenes/n_step_return_temporal_difference_learning.png\" width = \"400px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$).\n",
    "\n",
    "---\n",
    "\n",
    "Another approach to compute the value function is to average the $n$-step returns. The natural question that arrises from the latter proposal is: Can we efficiently combine information from all time-steps? The answer is **Yes, we can**, using the TD($\\lambda$) algorithm (we will incorporate this functionality in future versions of this notebook):\n",
    "\n",
    "<img src = \"./imagenes/lambda_return_temporal_difference.png\" width = \"500px\" >\n",
    "<img src = \"./imagenes/forward_view_lamda_return_temporal_difference.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
